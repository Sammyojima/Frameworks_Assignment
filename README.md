# ğŸ“Š CORD-19 Data Explorer

This project is part of the **Frameworks Assignment**. It explores the **CORD-19 dataset** (metadata of COVID-19 research papers) and builds a simple **Streamlit web app** for interactive visualization and analysis.  

---

## ğŸŒ Live Demo  
ğŸ”— [View the Live App Here](https://frameworksassignmentgit-v2e76wgayftb3bxvecdopd.streamlit.app/)  

---

## ğŸ¯ Project Purpose
- Practice **loading and exploring a real-world dataset**  
- Apply **data cleaning** and transformation techniques  
- Create **visualizations** with matplotlib/seaborn  
- Build an **interactive web application** with Streamlit  
- Deploy the project online for accessibility  

---

## ğŸ“‚ Project Structure
Frameworks_Assignment/
â”‚â”€â”€ app.py # Streamlit application
â”‚â”€â”€ analysis.ipynb # Jupyter notebook for exploration (optional)
â”‚â”€â”€ CORD19_metadata.csv # Dataset (metadata subset)
â”‚â”€â”€ requirements.txt # Dependencies for deployment
â”‚â”€â”€ screenshots
â”‚â”€â”€ README.md # Project documentation


---

## ğŸ“Š Features
- Load and explore **CORD-19 metadata dataset**  
- Handle missing data and extract publication years  
- Visualizations:
  - ğŸ“ˆ Number of publications over time  
  - ğŸ“Š Top publishing journals  
  - â˜ï¸ Word cloud of paper titles  
  - ğŸ” Distribution of papers by source  
- Interactive filters for year ranges  
- Simple **Streamlit interface**  

---

## ğŸ› ï¸ Tech Stack
- **Python 3.10+**  
- **pandas** â†’ data handling  
- **matplotlib & seaborn** â†’ visualizations  
- **Streamlit** â†’ web application framework  

---

## ğŸ“¸ Screenshots    

### Home Page  
![Home Screenshot](screenshots/Home.png)  

### Publications Over Time  
![Publications Screenshot](screenshots/publications.png)  

---

## ğŸš€ Getting Started  

### Clone the Repository
```bash
git clone https://github.com/Sammyojima/Frameworks_Assignment.git
cd Frameworks_Assignment

Install Dependencies
pip install -r requirements.txt

Run Locally
streamlit run app.py

ğŸ“ Reflection
This project improved my understanding of:
Data cleaning and handling missing values
Visual storytelling with charts
Building and deploying interactive web apps
End-to-end data science workflow
